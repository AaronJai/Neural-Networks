This is a "bare bones neural network implementation to describe the inner workings of backpropagation.


Part 1: A Tiny Toy Network
a neural network trained with backpropagation is attempting to use input to predict output.

Takeaways:
When both an input and a output are 1, we increase the weight between them. When an input is 1 and an output is 0, we decrease the weight between them

Part 2: A Harder Toy Network
implements a simple neural network with one hidden layer to learn a binary classification


Inspiration: [iamtrask](https://iamtrask.github.io/2015/07/12/basic-python-network/)